---
title: "GLUE environmental analyses: Predicting cyanogenesis clines"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
urlcolor: blue
---

```{r, include=FALSE}
# Load required packages
library(tidyverse)
library(sp)
library(rworldmap)
library(rworldxtra)
library(MASS)
library(heplots)
library(candisc)
library(ggord)
library(InPosition)
library(factoextra)
library(FactoMineR)
library(ggpubr)
library(gridExtra)
library(vegan)
library(wesanderson)
library(Hmisc)
library(glmnet)
library(caret)
source("../scripts/r/utilityFunctions.R")

# Theme used for plotting
ng1 <- theme(aspect.ratio=0.7,panel.background = element_blank(),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border=element_blank(),
          axis.line.x = element_line(color="black",size=1),
          axis.line.y = element_line(color="black",size=1),
          axis.ticks=element_line(color="black"),
          axis.text=element_text(color="black",size=15),
          axis.title=element_text(color="black",size=1),
          axis.title.y=element_text(vjust=2,size=17),
          axis.title.x=element_text(vjust=0.1,size=17),
          axis.text.x=element_text(size=15),
          axis.text.y=element_text(size=15),
          strip.text.x = element_text(size = 10, colour = "black",face = "bold"),
          strip.background = element_rect(colour="black"),
          legend.position = "top", legend.direction="vertical",
          legend.text=element_text(size=17), legend.key = element_rect(fill = "white"),
          legend.title = element_text(size=17),legend.key.size = unit(1.0, "cm"))

# Correlation panel
panel.cor <- function(x, y){
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- round(cor(x, y), digits=2)
  txt <- paste0("R = ", r)
  cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt)
}
# Customize upper panel
upper.panel<-function(x, y){
  points(x,y, pch = 19)
  abline(lm(y~x), col='red', lwd = 2)
}
```

# Setup

## Load in data

`df_all_popMeans` is a concatenated dataset with the population-mean HCN frequencies and environmental data for every city. 
  
```{r}
# Create dataframe with population-mean HCN and environmental variables for every city
inpath <- "../data/clean/popMeans_allCities_withEnviro/"
csv.files <- list.files(path = inpath, pattern="*.csv")
df_all_popMeans <- c()
for (i in 1:length(csv.files)){
  data <- read.csv(paste0(inpath, csv.files[i])) %>% dplyr::select(city, 
                                                                   std_distance, 
                                                                   freqHCN, 
                                                                   matches("*Mean$"))
  df_all_popMeans <- rbind(df_all_popMeans, data)
}

df_all_popMeans <- df_all_popMeans %>% 
  dplyr::filter(!(city == "St_Albert"))
```

## Set up data for use in regression

The code below creates the data frames and matrices that will be used for analyses.

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
# Run function that returns matrix of slopes of environmental variables for each city
results_statsMatrices <- calculate.stats(all.data = df_all_popMeans, 
                                         permute = FALSE, 
                                         number.extreme.sites=2)

# Extract slopes of environmental variables.
envSlopes <- results_statsMatrices$slope.matrix %>% 
  dplyr::select(-city) %>% 
  setNames(names(.) %>% stringr::str_replace("Mean", "Slope")) %>% 
  as.matrix()

# Extract mean value of environmental variables for each city
envMeans <- calculate_city_eviro_means(df_all_popMeans) %>% 
  dplyr::select(-city) %>% 
  drop_na() %>% 
  setNames(paste0(names(.), "_Mean")) %>% 
  as.matrix()

# Get Slopes of HCN clines from Robust regression
HCNslopes <- slope.freqHCN(all.data = df_all_popMeans, number.extreme.sites=1)
HCNslopes <- HCNslopes$slopes %>% 
  as.data.frame() %>% 
  dplyr::select(rlm) %>%
  rename("HCNslope" = "rlm") %>% 
  as.matrix()

# Distance vector: Distance between urban and rural multivariate environments for each city
distance_vector <- results_statsMatrices$D.UR

# Create dataframe with HCN slopes and environmental data
df_slopes_enviro <- data.frame(HCNslopes, envSlopes, envMeans)
```

# Analyses

We had previously used __LASSO regression__ to examine the environmental predictors of HCN clines. LASSO regularizes regression coefficients by penalizing the sum of their absolute values; unimportant predictors may have their coefficients set to 0, which makes LASSO useful for model selection. The magnitude of this penalization is controlled through the shrinkage parameter $\lambda$; When $\lambda$ = 0, coefficients are estimated via OLS. However, when two predictors are highly colinear, LASSO is known to just randomly pick one of the colinear predictors. 

__Ridge regression__, on the other hand, penalizes the sum of the squared regression coefficients. As a result, colinear predictors have their coefficients shrunk _towards one another_ so their estimated effects on the response are the similar. However, Ridge regression coefficients will only ever approach, but never equal, zero so it can't be used for variable selection. The magnitude of penalization is still controlled through the shrinkage parameter $\lambda$.

__Elastic Net__ combines both LASSO and Ridge regression, and is meant to overcome the limitations of both. Elastic net still uses the shrinkage parameter $\lambda$ to control the magnitude of penalization to the regression coefficients, but additionally include the _mixing parameter_ $\alpha$, which controls the extent to which the regression is more ridge vs. more lasso; when $\alpha$ = 0, this is equivalent to Ridge regression, whereas $\alpha$ = 1 is equivalent to LASSO. Anything in between is elastic net, which allows coefficients to be set to 0 (I.e., useful for variable selection) and is more likely to include both colinear predictors rather than choosing one at random (like LASSO).

For elastic net, both $\lambda$ and $\alpha$ are chosen through cross-validation. 

__Note:__ I suspect Elastic Net is most useful for moderately correlated predictors. If predictors are highly correlated, I think elastic net will still struggle to include both terms, especially if $\alpha$ is closer to 1. This is more of a gut feeling but is likely relevant for our data (see below).

## Correlations among predictor variables

As you can see below, many of the environmental variables are at least moderately correlated (0.4 < R < 0.6) whereas other are highly correlated. For example, the pearson correlation coefficient between `winterNDVI_Mean` and `NDSI_Mean` is -0.93, which is super high. I don't think even elastic net can distinguish among these.

### Correlations among mean environmental variables

```{r, message=FALSE}
pairs(envMeans, 
      lower.panel = panel.cor,
      upper.panel = upper.panel)
```

### Correlations among the slopes of environmental variables

__Note:__ St-Albert was previously identified as an outlier in `GMIS_Slope` and has been removed from the analyses. The outlier was due to St-Albert missing GMIS data for the rural half of its transect so the calculated slope was very large. 

```{r}
pairs(envSlopes, 
      lower.panel = panel.cor,
      upper.panel = upper.panel)
```


## Elastic Net

As Marc and I previously discussed, we are not including all possible interactions. Specifically, we are primarily interested in how changes in the environment across urban-rural transects influence changes in HCN. Because urban-rural changes in one environmental factor might depend on changes in another, we are including interactions between the _slopes_ of two environmental variables. In addition, because changes in one environmental variable may depend on regional environmental conditions, we are including interactions involving the _slopes_ of one environmental variable and the _mean_ of another. We are not specifically interested in how regional environments affect urban-rural clines in HCN so we are __not__ including interactions between the means of two environmental variables. Finally, we are only including two-way interactions.

With the criteria outlined above, we have a total of **136 predictors**.

```{r}
# Create Matrix with predictors (including interactions)
# Do not include interactions involving distance vector
# Do not include predictors with interactions among mean environmental variables
predictors <- cbind(distance_vector, envSlopes, envMeans)
predictors_withInteractions <- model.matrix( ~.^2, data = data.frame(scale(predictors)))[,-1]  # remove intercept
predictors_withInteractions_reduced <- predictors_withInteractions %>% 
  as.data.frame() %>% 
  dplyr::select(-contains(':'),
                contains('Slope:')) %>% 
  as.matrix()
model_matrix <- cbind(HCNslopes, predictors_withInteractions_reduced)
```

The code below performs the elastic net. For each of 10 values of $\alpha$ crossed with 10 values of $\lambda$ (total N = 100), the model uses 10-fold cross validation to calculate the mean Root Mean Squared Error (RMSE) of the model for that combination of $\alpha$ and $\lambda$. The combination is with the lowest RMSE is chosen.


```{r}
set.seed(12)
elasticNet_model <- caret::train(
  y = model_matrix[,1], # HCNslopes
  x = model_matrix[,-1], # All predictores
  method = "glmnet",
  metrix = "RMSE",
  trControl = trainControl("cv", number = 10, savePredictions = "all"),
  tuneLength = 10
)

# Full list of results
elasticNet_modelResults <- data.frame(elasticNet_model$results)

# Best tuning parameter
print(elasticNet_model$bestTune)
```

Here is a plot of the RMSE for different combinations of $\alpha$ and $\lambda$

```{r}
ggplot(data = elasticNet_modelResults, aes(x = lambda, y = RMSE, colour = as.factor(alpha))) + 
  geom_point(size = 2) + 
  geom_line() +
  scale_color_discrete(name = "alpha") +
  ng1 + theme(legend.position = "top",
              legend.direction = "horizontal") 
```

And here are the results of the final model:

```{r}
bestAlpha <- elasticNet_model$bestTune$alpha
bestLambda <- elasticNet_model$bestTune$lambda

# Final model
EN_finalModel <- elasticNet_model$finalModel

# Extract nonzero coefficients
predictors_nonZero <- which(coef(EN_finalModel, bestLambda)!=0)[2:length(which(coef(EN_finalModel, bestLambda)!=0))]-1
predictors_finalModel <- predictors_withInteractions_reduced[, predictors_nonZero]

model_df = as.data.frame(cbind(HCNslopes, predictors_finalModel))

# Run final model
finalModel <- lm(scale(HCNslopes) ~ ., data = model_df[-1])
summary(finalModel)
```

Here are the **Variance Inflation Factors** for predictors in the model above. The square root of VIF is interpreted as how much the SE of a variable's coefficient is increased relative to a model where the variable had zero correlation with any other predictors in the model. Larger SE for coefficients reduce our ability to detect significant effects, so lower VIF is better (e.g., < 4). The VIF's below don't seem that problematic since the only variable with high VIF are interactions, which we expect to be higher since they are formed by combinations of main effects. 

```{r}
vif(finalModel)
```

**Potential problem:** Some of the interactions in the model above do not have their main effects included in the model. LASSO (and by extension Elastic Net) seems to select interactions even when one or both main effects are set to 0 (i.e., estimated as having no effect). This may not be a problem for traditional LASSO but I wonder if it is in our case since we're re-fitting the model using OLS and care about estimated coefficients; I'm not use to having models violate the Hierarchy Principle. 

## Plot of important predictors 

Here are a couple of plots of significant main effects. I am not plotting the interactions, which I think we could do with coloured contour plots or heatmap-style plots.

### Winter NDVI

**Important Note:** Remember that winter NDVI is **strongly** correlated with NDSI such that these probably can't be interpreted on their own and I don't think even elastic net with tease these apart. Basically, the plot below would suggest that we get weaker clines in cities less winter vegetation/more snow.

```{r}
ggplot(data = data.frame(model_matrix), aes(x = winterNDVI_Mean, y = HCNslopes)) + 
  geom_point(size = 2) +
  geom_smooth(method = 'lm', colour = "black") +
  xlab("Mean winter NDVI") + 
  ylab("Standardized slope of HCN cline") +
  ng1
```

### Slope of PET

Just throwing this here for posterity. PET is involved in two significant interactions so we probably wouldn't want to interpret this on its own.

```{r}
ggplot(data = data.frame(model_matrix), aes(x = annualPET_Slope, y = HCNslopes)) + 
  geom_point(size = 2) +
  geom_smooth(method = 'lm', colour = "black") +
  xlab("Slope of annual PET") + 
  ylab("Standardized slope of HCN cline") +
  ng1
```

## Sensitivity of model to multicolinearity

Marc suggested performing a sort of "sensitivy analysis" to assess how multicolinearity may be influencing the results described in the model above. To do this, we will repeat the Elastic Net model selection, removing non-significant effects that are highly correlated (r > 0.7) with significant effects detected in the model above. By this criteria, we will remove effects containing `GMIS_Mean` and `NDSI_Mean`.

By this criteria, the number of predictors is now **116**

```{r}
# Create Matrix with predictors (including interactions)
# Do not include interactions involving distance vector
# Do not include predictors with interactions among mean environmental variables
# Do not include variables with GMIS_Mean or NDSI_Mean
predictors_colinTest <- cbind(distance_vector, envSlopes, envMeans)
predictors_withInteractions_colinTest <- model.matrix( ~.^2, data = data.frame(scale(predictors_colinTest)))[,-1]  # remove intercept
predictors_withInteractions_reduced_colinTest <- predictors_withInteractions_colinTest %>% 
  as.data.frame() %>% 
  dplyr::select(-contains(':'),
                contains('Slope:'),
                -contains('NDSI_Mean'),
                -contains('GMIS_Mean')) %>% 
  as.matrix()
model_matrix_colinTest <- cbind(HCNslopes, predictors_withInteractions_reduced_colinTest)
```

Repeat the Elastic Net

```{r}
set.seed(12)
elasticNet_model_colinTest <- caret::train(
  y = model_matrix_colinTest[,1], # HCNslopes
  x = model_matrix_colinTest[,-1], # All predictores
  method = "glmnet",
  metrix = "RMSE",
  trControl = trainControl("cv", number = 10, savePredictions = "all"),
  tuneLength = 10
)

# Full list of results
elasticNet_modelResults_colinTest<- data.frame(elasticNet_model_colinTest$results)

# Best tuning parameter
print(elasticNet_model_colinTest$bestTune)
```

```{r}
bestAlpha_colinTest <- elasticNet_model_colinTest$bestTune$alpha
bestLambda_colinTest <- elasticNet_model_colinTest$bestTune$lambda

# Final model
EN_finalModel_colinTest <- elasticNet_model_colinTest$finalModel

# Extract nonzero coefficients
predictors_nonZero_colinTest <- which(coef(EN_finalModel_colinTest, bestLambda_colinTest)!=0)[2:length(which(coef(EN_finalModel_colinTest, bestLambda_colinTest)!=0))]-1
predictors_finalModel_colinTest <- predictors_withInteractions_reduced_colinTest[, predictors_nonZero_colinTest]

model_df_colinTest = as.data.frame(cbind(HCNslopes, predictors_finalModel_colinTest))

# Run final model
finalModel_colinTest <- lm(scale(HCNslopes) ~ ., data = model_df_colinTest[-1])
summary(finalModel_colinTest)
```

Model is very consistent with the previous one, suggesting multicolinearity may not have been that big of a problem in allowing the Elastic Net to come up with a reduced predictor set. VIF beow also look good. They are smaller due to the removal of highly correlated variables 

```{r}
vif(finalModel_colinTest)
```


